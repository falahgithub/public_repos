{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTJs9z+vwlIJfuDQ0MfMfZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falahgithub/public_repos/blob/main/Loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * Steps that are followed in following program *\n",
        "\n",
        "### Step 0 : Saving CSV files into the database\n",
        "### Step 1 : Importing files from the database and converting them into dataframes\n",
        "### Step 2 : Checking whether all the dataframes contains required columns\n",
        "### Step 3 : Analyzing dataframes along with Data Cleaning and Manipulation\n",
        "### Step 4 : Calculating threshold to filter rows of dataframe depending upon situations\n",
        "### Step 5 : Data calculations to generate results"
      ],
      "metadata": {
        "id": "xDNqOJQ1yY4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to save data in the database"
      ],
      "metadata": {
        "id": "06e3HIM9gpYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def files_to_database():\n",
        "  # Read CSV data into a DataFrame\n",
        "\n",
        "  df1_export = pd.read_csv('store status.csv')\n",
        "  df2_export = pd.read_csv('Menu hours.csv')\n",
        "  df3_export = pd.read_csv('bq-results-20230125-202210-1674678181880.csv')\n",
        "\n",
        "  # Connect to SQLite database\n",
        "\n",
        "  conn = sqlite3.connect('mydb.db')\n",
        "\n",
        "  # Insert data into a table\n",
        "\n",
        "  df1_export.to_sql('T1', conn, if_exists='replace')\n",
        "  df2_export.to_sql('T2', conn, if_exists='replace')\n",
        "  df3_export.to_sql('T3', conn, if_exists='replace')\n",
        "\n",
        "  # Close the database connection\n",
        "\n",
        "  conn.close()"
      ],
      "metadata": {
        "id": "NNB8wz6vgo9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to import tables from database"
      ],
      "metadata": {
        "id": "AtCekS41mmBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def importing_tables():\n",
        "\n",
        "  \"\"\" This function imports CSV files from the database and converts them into the dataframes.\"\"\"\n",
        "\n",
        "  # Connect to the database\n",
        "\n",
        "  conn = sqlite3.connect('mydb.db')\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "  # Export the first table\n",
        "\n",
        "  cursor.execute('SELECT * FROM T1')\n",
        "  rows = cursor.fetchall()\n",
        "  with open('table1.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow([description[0] for description in cursor.description])\n",
        "      writer.writerows(rows)\n",
        "\n",
        "  # Export the second table\n",
        "\n",
        "  cursor.execute('SELECT * FROM T2')\n",
        "  rows = cursor.fetchall()\n",
        "  with open('table2.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow([description[0] for description in cursor.description])\n",
        "      writer.writerows(rows)\n",
        "\n",
        "  # Export the third table\n",
        "\n",
        "  cursor.execute('SELECT * FROM T3')\n",
        "  rows = cursor.fetchall()\n",
        "  with open('table3.csv', 'w', newline='') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow([description[0] for description in cursor.description])\n",
        "      writer.writerows(rows)\n",
        "\n",
        "  # Close the database connection\n",
        "\n",
        "  conn.close()\n",
        "\n",
        "  # Converting CSV to Dataframes\n",
        "\n",
        "  df1 = pd.read_csv(\"table1.csv\").drop(\"index\", axis=1)\n",
        "  df2 = pd.read_csv(\"table2.csv\").drop(\"index\", axis=1)\n",
        "  df3 = pd.read_csv(\"table3.csv\").drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "XMj_kFiMju_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking whether all the dataframes contains required columns"
      ],
      "metadata": {
        "id": "FE-F6GdyAnHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checking_imports(*dfs: pd.DataFrame): \n",
        "  \n",
        "  # Validate input data for each DataFrames\n",
        "\n",
        "  for df in dfs:\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "      raise TypeError(\"All dataframes must be pandas DataFrames.\")\n",
        "\n",
        "    if not {'store_id'}.issubset(df.columns):\n",
        "       raise ValueError(\"All dataframes must contain 'store_id' column.\")"
      ],
      "metadata": {
        "id": "iY7tsc7MBc9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing dataframes along with Data Cleaning and Manipulation"
      ],
      "metadata": {
        "id": "Y18z5qdyAshF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyz_clean_tables(df1, df2, df3):\n",
        "  \n",
        "  \"\"\"This function analyzes all the dataframes, perform data cleaning function and finally manipulate dataframes as required by the problem. It takes dataframes as input.\"\"\" \n",
        "  \n",
        "  # Data information\n",
        "\n",
        "  df1.info(), df2.info(), df3.info()\n",
        "\n",
        "  # Merge dataframe on \"store_id\"\n",
        "\n",
        "  df13 = pd.merge(df1, df3, how=\"outer\", on=\"store_id\")\n",
        "\n",
        "  # Data Cleaning\n",
        "\n",
        "  df13.fillna({\"timezone_str\": \"America/Chicago\"}, inplace=True)\n",
        "  df13 = df13.dropna()\n",
        "\n",
        "  # Data manipulation\n",
        "\n",
        "  df13[\"timestamp_utc\"] = df13[\"timestamp_utc\"].apply(lambda x: x.replace(\" UTC\", \"\"))\n",
        "  df13[\"timestamp_utc\"] = pd.to_datetime(df13[\"timestamp_utc\"], utc = True)\n",
        "  df13[\"timestamp_local\"] = df13.apply(lambda row: row[\"timestamp_utc\"].tz_convert(row[\"timezone_str\"]), axis=1)          # remember axisdf13\n",
        "  df13[\"day\"] = df13[\"timestamp_local\"].apply(lambda x: x.dayofweek)       \n",
        "\n",
        "  # Merge all dataframes on \"store_id\"\n",
        "\n",
        "  df123 = pd.merge(df13, df2,how=\"left\" , on=[\"store_id\", \"day\"])   \n",
        "\n",
        "  # Data Cleaning\n",
        "\n",
        "  df123 = df123.fillna({\"start_time_local\": \"00:00:00\",\"end_time_local\": \"23:59:59\"})\n",
        "  df123[\"end_time_local\"].replace(to_replace=\"00:00:00\", value=\"23:59:59\", inplace=True)  \n",
        "  \n",
        "  return df123\n",
        "  "
      ],
      "metadata": {
        "id": "UiRELemvzVHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating threshold to filter rows of dataframe depending upon situations"
      ],
      "metadata": {
        "id": "6IJZyoaEAwhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_calc(df):\n",
        "  \n",
        "  \"\"\"This function is used to calculate threshold to filter rows of dataframe depending upon situations. Here 1 hr, 1 day, and 1 week.\"\"\"\n",
        "\n",
        "  # Thresholds calculation to get desired rows\n",
        "\n",
        "  end_th = df['timestamp_utc'].max()\n",
        "  hour_th = df['timestamp_utc'].max() - pd.Timedelta(hours=1)\n",
        "  day_th = df['timestamp_utc'].max() - pd.Timedelta(days=1)\n",
        "  week_th = df['timestamp_utc'].max() - pd.Timedelta(weeks=1)\n",
        "\n",
        "  return end_th, hour_th, day_th, week_th          "
      ],
      "metadata": {
        "id": "JYzT0_VZCQdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data calculations to generate results"
      ],
      "metadata": {
        "id": "OklzxTaNA4ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def proces_group_data(data, end_th, hour_th, day_th, week_th):\n",
        "  \n",
        "  \"\"\"This function filters rows of dataframe depending upon the threshold calculated. It returns uptime/downtime calculated.\"\"\"\n",
        "    \n",
        "  last_hour = data[data['timestamp_utc'] > hour_th]        # filtering rows within an hour\n",
        "  last_day = data[data['timestamp_utc'] > day_th]          # filtering rows within a day\n",
        "  last_week = data[data['timestamp_utc'] > week_th]        # filtering rows within a week\n",
        "\n",
        "  u_h, d_h = calvall(last_hour, hour_th, end_th)  \n",
        "  u_d, d_d = calvall(last_day, day_th, end_th)     \n",
        "  u_w, d_w = calvall(last_week, week_th, end_th)  \n",
        "\n",
        "  return u_h, u_d, u_w, d_h, d_d, d_w  "
      ],
      "metadata": {
        "id": "ZmHj4kfaY1vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calvall(data, start_threshold, end_threshold):\n",
        "\n",
        "  \"\"\"This function calculates uptime/downtime. It returns uptime and downtime values.\"\"\"\n",
        "  \n",
        "  uptime = 0\n",
        "  downtime = 0\n",
        " \n",
        "  if data.empty:\n",
        "    pass\n",
        "  \n",
        "  else:\n",
        "    newgroups = data.groupby(\"day\")                                                        # Grouping dataframe on different days\n",
        "\n",
        "    for gr_name, gr_data in newgroups: \n",
        "      if gr_data.empty:\n",
        "        pass\n",
        "      else:\n",
        "        \n",
        "        timezone = gr_data[\"timezone_str\"].iloc[0]                                          # timezone identification\n",
        "        \n",
        "        \n",
        "        start_cutoff = start_threshold.tz_convert(timezone)                                 # For cutoff limit\n",
        "        end_cutoff = end_threshold.tz_convert(timezone)                                     # For cutoff limit\n",
        "      \n",
        "        \n",
        "        s_time = pd.to_datetime(gr_data[\"start_time_local\"].iloc[0]).time()                  # Business hours start time\n",
        "        e_time = pd.to_datetime(gr_data[\"end_time_local\"].iloc[0]).time()                    # Business hours end time\n",
        "        starting_hr = int(s_time.strftime(\"%H\"))                                             # Extracting hours from start time\n",
        "        ending_hr = int(e_time.strftime(\"%H\"))                                               # Extracting hours from end time\n",
        "\n",
        "        \n",
        "        gr_data = gr_data.sort_values(\"timestamp_local\")                                     # sorting dataframe\n",
        "        gr_data[\"status_code\"] = gr_data[\"status\"].replace({\"active\": 1, \"inactive\": 0})\n",
        "        gr_data = gr_data[[\"timestamp_local\", \"status_code\"]]    \n",
        "        gr_data = gr_data.set_index(\"timestamp_local\")                                        \n",
        "        gr_data = gr_data.resample(\"1H\").max()                                                # Resampling data\n",
        "        \n",
        "      \n",
        "       \n",
        "        start_pos = gr_data.index[0].replace(hour=starting_hr, minute = 0, second = 0)         # Business start time with current date\n",
        "        end_pos = gr_data.index[0].replace(hour=ending_hr, minute = 0, second = 0)             # Business end time with current date\n",
        "\n",
        "\n",
        "        gr_data = gr_data.reindex(pd.date_range(start_pos, end_pos, freq='1H'))                # Reindexing\n",
        "        \n",
        "        \n",
        "        try:\n",
        "          gr_data = gr_data.interpolate(method= \"time\", limit_direction=\"both\")\n",
        "        \n",
        "        except:\n",
        "          gr_data = gr_data.fillna({\"status_code\":1})\n",
        "        \n",
        "        \n",
        "        gr_data = gr_data.query('@start_cutoff <= index <= @end_cutoff')                       # Filtering dataframe within cutoff limits\n",
        "                  \n",
        "        for index, row in gr_data.iterrows():\n",
        "          if row[\"status_code\"] > 0.66 :\n",
        "             uptime += 1\n",
        "          else:\n",
        "            downtime += 1   \n",
        "  \n",
        "  return uptime, downtime"
      ],
      "metadata": {
        "id": "L68M1CeQNV4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty lists to store value for output schema\n",
        "def main():\n",
        "\n",
        "  \"\"\"This function is responsible for coordinating between different functions.\"\"\"\n",
        "  \n",
        "  files_to_database()\n",
        "  df1, df2, df3 = importing_tables()                                      # Step 1\n",
        "  checking_imports(df1, df2, df3)                                         # Step 2\n",
        "  df = analyz_clean_tables(df1, df2, df3)                                 # Step 3\n",
        "  end_th, hour_th, day_th, week_th = threshold_calc(df)                   # Step 4\n",
        "\n",
        "\n",
        "  store = []\n",
        "  u_h_list = []\n",
        "  u_d_list = [] \n",
        "  u_w_list = []\n",
        "  d_h_list = []\n",
        "  d_d_list = [] \n",
        "  d_w_list = []\n",
        "\n",
        "  # Create groups object\n",
        "\n",
        "  groups = df.groupby(\"store_id\")\n",
        "\n",
        "  for name, data in groups:\n",
        "    store.append(name)  \n",
        "    u_h, u_d, u_w, d_h, d_d, d_w  = proces_group_data(data, end_th, hour_th, day_th, week_th)\n",
        "    u_h_list.append(u_h*60)                # 60 as output is required in minutes\n",
        "    u_d_list.append(u_d) \n",
        "    u_w_list.append(u_w)\n",
        "    d_h_list.append(d_h*60)                # 60 as output is required in minutes\n",
        "    d_d_list.append(d_d) \n",
        "    d_w_list.append(d_w) \n",
        "\n",
        "\n",
        "  # Required output dataframe\n",
        "\n",
        "  dfgen = pd.DataFrame({\"store_id\": store,\n",
        "                      \"uptime_last_hour(in minutes)\":u_h_list,\n",
        "                      \"uptime_last_day(in hours)\": u_d_list,\n",
        "                      \"uptime_last_week(in hours)\" : u_w_list,\n",
        "                      \"downtime_last_hour(in minutes)\": d_h_list,\n",
        "                      \"downtime_last_day(in hours)\": d_d_list,\n",
        "                      \"downtime_last_week(in hours)\": d_w_list,\n",
        "                      })\n",
        "  \n",
        "  dfgen.to_csv(\"result.csv\", index=False)"
      ],
      "metadata": {
        "id": "grscu43bBJR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import sqlite3\n",
        "import csv\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "Z6JBnKOEI-JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = pd.read_csv(\"result.csv\")\n",
        "res"
      ],
      "metadata": {
        "id": "1MVCW1zGSEUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}